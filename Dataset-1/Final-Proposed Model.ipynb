{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957ffcc-4b78-4cb9-927d-67da6af9499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from scipy.signal import resample, correlate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, cohen_kappa_score, matthews_corrcoef, log_loss, roc_auc_score, average_precision_score\n",
    "\n",
    "# Load datasets\n",
    "# Load datasets\n",
    "eeg_data = pd.read_csv('eeg_data_trimmed5.csv')\n",
    "emg_data = pd.read_csv('emg_data_trimmed5.csv')\n",
    "labels = pd.read_csv('labels_trimmed5.csv')\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels.values.ravel())\n",
    "\n",
    "# Resample EEG to match EMG's length (if necessary)\n",
    "if len(eeg_data) != len(emg_data):\n",
    "    eeg_data = resample(eeg_data, num=len(emg_data))\n",
    "\n",
    "# Sliding Window Cross-Correlation for Real-Time Alignment\n",
    "def sliding_window_cross_correlation(eeg, emg, window_size=100, overlap=50):\n",
    "    eeg_aligned = np.zeros_like(eeg)\n",
    "    emg_aligned = np.zeros_like(emg)\n",
    "    step = window_size - overlap\n",
    "    sync_scores = []  # To store synchronization scores for each window\n",
    "\n",
    "    for i in range(0, len(eeg) - window_size, step):\n",
    "        eeg_window = eeg[i:i + window_size].flatten()  # Flatten the EEG window\n",
    "        emg_window = emg[i:i + window_size].flatten()  # Flatten the EMG window\n",
    "\n",
    "        # Normalize the signals (zero mean and unit variance)\n",
    "        eeg_window = (eeg_window - np.mean(eeg_window)) / np.std(eeg_window)\n",
    "        emg_window = (emg_window - np.mean(emg_window)) / np.std(emg_window)\n",
    "\n",
    "        # Compute cross-correlation\n",
    "        correlation = correlate(eeg_window, emg_window, mode='full')\n",
    "        lags = np.arange(-len(eeg_window) + 1, len(emg_window))\n",
    "\n",
    "        # Normalize the cross-correlation values\n",
    "        correlation = correlation / (np.linalg.norm(eeg_window) * np.linalg.norm(emg_window))\n",
    "\n",
    "        # Find the lag with the maximum correlation\n",
    "        lag = lags[np.argmax(correlation)]\n",
    "        max_corr = np.max(correlation)  # Maximum correlation value (synchronization score)\n",
    "        sync_scores.append(max_corr)\n",
    "\n",
    "        # Debug: Print lag and max correlation\n",
    "        print(f\"Window {i}: Lag = {lag}, Max Correlation = {max_corr:.4f}\")\n",
    "\n",
    "        # Align signals within the window\n",
    "        if lag > 0:\n",
    "            eeg_aligned[i:i + window_size] = np.roll(eeg[i:i + window_size], lag, axis=0)\n",
    "            emg_aligned[i:i + window_size] = emg[i:i + window_size]\n",
    "        else:\n",
    "            eeg_aligned[i:i + window_size] = eeg[i:i + window_size]\n",
    "            emg_aligned[i:i + window_size] = np.roll(emg[i:i + window_size], -lag, axis=0)\n",
    "\n",
    "    # Average synchronization score across all windows\n",
    "    avg_sync_score = np.mean(sync_scores)\n",
    "    print(f\"Average Synchronization Score: {avg_sync_score:.4f}\")\n",
    "\n",
    "    return eeg_aligned, emg_aligned, avg_sync_score\n",
    "\n",
    "\n",
    "# Apply sliding window cross-correlation\n",
    "eeg_aligned, emg_aligned, sync_score = sliding_window_cross_correlation(eeg_data.values, emg_data.values)\n",
    "\n",
    "# Combine aligned data\n",
    "combined_data = pd.concat([pd.DataFrame(eeg_aligned), pd.DataFrame(emg_aligned)], axis=1)\n",
    "\n",
    "# Define a custom Dataset\n",
    "class EEGEMGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Transformer model with real-time synchronization and cross-modality attention\n",
    "class EEGEMGTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_classes, dropout_rate=0.5):\n",
    "        super(EEGEMGTransformer, self).__init__()\n",
    "        self.align_eeg = nn.Linear(input_dim, input_dim)\n",
    "        self.align_emg = nn.Linear(input_dim, input_dim)\n",
    "        \n",
    "        self.eeg_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, batch_first=True, dropout=dropout_rate),\n",
    "            num_layers=2)\n",
    "        self.emg_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, batch_first=True, dropout=dropout_rate),\n",
    "            num_layers=2)\n",
    "        \n",
    "        self.eeg_projector = nn.Linear(input_dim, hidden_dim)\n",
    "        self.emg_projector = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        # Initialize cross-modality attention weights with higher weight for EEG\n",
    "        self.cross_attention_weights = nn.Parameter(torch.tensor([[0.7], [0.3]]), requires_grad=True)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, eeg, emg):\n",
    "        eeg = self.align_eeg(eeg)\n",
    "        emg = self.align_emg(emg)\n",
    "        eeg_features = self.eeg_encoder(eeg)\n",
    "        emg_features = self.emg_encoder(emg)\n",
    "        \n",
    "        eeg_features = self.eeg_projector(eeg_features)\n",
    "        emg_features = self.emg_projector(emg_features)\n",
    "        \n",
    "        eeg_features = self.dropout(eeg_features)\n",
    "        emg_features = self.dropout(emg_features)\n",
    "        \n",
    "        # Weighted combination of EEG and EMG\n",
    "        combined_features = (\n",
    "            self.cross_attention_weights[0] * eeg_features + self.cross_attention_weights[1] * emg_features\n",
    "        )\n",
    "        combined, _ = self.cross_attention(combined_features, combined_features, combined_features)\n",
    "        \n",
    "        output = self.fc(combined.mean(dim=1))\n",
    "        return output, self.cross_attention_weights\n",
    "\n",
    "\n",
    "# Dynamic Learning Rate Adjustment\n",
    "def online_adaptation_with_regularization(\n",
    "    model, optimizer, buffer_X, buffer_y, criterion, val_loader, num_cycles=2, batch_size=8, lr=0.00001\n",
    "):\n",
    "    model.train()\n",
    "    \n",
    "    # Create a replay buffer with original training data\n",
    "    replay_buffer_X = X_train[:100]  # Store 100 samples from training data\n",
    "    replay_buffer_y = y_train[:100]\n",
    "    buffer_X = np.concatenate([buffer_X, replay_buffer_X])\n",
    "    buffer_y = np.concatenate([buffer_y, replay_buffer_y])\n",
    "    \n",
    "    buffer_dataset = EEGEMGDataset(buffer_X, buffer_y)\n",
    "    buffer_loader = DataLoader(buffer_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Lower learning rate for stable online updates\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    # Learning rate scheduler with warm-up\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=lr/10, max_lr=lr, step_size_up=5, mode='triangular2')\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = model.state_dict()\n",
    "\n",
    "    for cycle in range(num_cycles):\n",
    "        print(f\"Online Adaptation Cycle {cycle + 1}/{num_cycles}\")\n",
    "        \n",
    "        # Train on adaptation data\n",
    "        for X_batch, y_batch in buffer_loader:\n",
    "            eeg = X_batch[:, :input_dim].unsqueeze(1)\n",
    "            emg = X_batch[:, input_dim:].unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(eeg, emg)  # Ignore attention weights during training\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Evaluate on the validation set after each cycle\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                eeg = X_batch[:, :input_dim].unsqueeze(1)\n",
    "                emg = X_batch[:, input_dim:].unsqueeze(1)\n",
    "                outputs, _ = model(eeg, emg)  # Ignore attention weights during validation\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation Loss After Cycle {cycle + 1}: {val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            print(\"New best model saved.\")\n",
    "        else:\n",
    "            print(\"Validation loss increased. Early stopping triggered.\")\n",
    "            break  # Stop adaptation if validation loss increases\n",
    "\n",
    "    # Load the best model state after adaptation\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Best model loaded after online adaptation.\")\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "def plot_roc_curve(y_true, y_scores, num_classes):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    y_true = np.array(y_true)  # Ensure y_true is a NumPy array\n",
    "    \n",
    "    # One-vs-Rest ROC Curve for multiclass classification\n",
    "    for i in range(num_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_true == i, y_scores[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower center\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_scores, num_classes):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    y_true = np.array(y_true)  # Ensure y_true is a NumPy array\n",
    "    \n",
    "    # One-vs-Rest Precision-Recall Curve for multiclass classification\n",
    "    for i in range(num_classes):\n",
    "        precision, recall, _ = precision_recall_curve((y_true == i).astype(int), y_scores[:, i])\n",
    "        pr_auc = auc(recall, precision)\n",
    "        plt.plot(recall, precision, label=f'Class {i} (AUC = {pr_auc:.2f})')\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower center\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "def calculate_hamming_loss(y_true, y_pred):\n",
    "    return hamming_loss(y_true, y_pred)\n",
    "\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "def calculate_top_k_accuracy(y_true, y_scores, k=3):\n",
    "    return top_k_accuracy_score(y_true, y_scores, k=k)\n",
    "\n",
    "def analyze_attention_weights(attention_weights):\n",
    "    avg_attention_weights = np.mean(attention_weights, axis=0)\n",
    "    print(f\"Average Attention Weights - EEG: {avg_attention_weights[0][0]:.4f}, EMG: {avg_attention_weights[1][0]:.4f}\")\n",
    "    return avg_attention_weights\n",
    "\n",
    "def plot_learning_curves(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def ablation_study(model, val_loader, ablation_type='eeg'):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            eeg = X_batch[:, :input_dim].unsqueeze(1)\n",
    "            emg = X_batch[:, input_dim:].unsqueeze(1)\n",
    "            \n",
    "            if ablation_type == 'eeg':\n",
    "                eeg = torch.zeros_like(eeg)  # Ablate EEG\n",
    "            elif ablation_type == 'emg':\n",
    "                emg = torch.zeros_like(emg)  # Ablate EMG\n",
    "            \n",
    "            outputs, _ = model(eeg, emg)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Ablation Study ({ablation_type.upper()} ablated): Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "import time\n",
    "\n",
    "def measure_inference_time(model, val_loader):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            eeg = X_batch[:, :input_dim].unsqueeze(1)\n",
    "            emg = X_batch[:, input_dim:].unsqueeze(1)\n",
    "            outputs, _ = model(eeg, emg)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
    "    return inference_time\n",
    "\n",
    "def measure_model_size(model):\n",
    "    param_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model Size: {param_size} parameters\")\n",
    "    return param_size\n",
    "\n",
    "\n",
    "def compute_average_metrics(metrics_list):\n",
    "    average_metrics = {}\n",
    "    for metric in metrics_list[0].keys():\n",
    "        values = [result[metric] for result in metrics_list if result[metric] is not None]  # Filter out None values\n",
    "        if values:  # Only compute the average if there are valid values\n",
    "            average_metrics[metric] = np.mean(values)\n",
    "        else:\n",
    "            average_metrics[metric] = None  # Set to None if no valid values exist\n",
    "    return average_metrics\n",
    "\n",
    "\n",
    "\n",
    "def analyze_attention_weights(attention_weights):\n",
    "    avg_attention_weights = np.mean(attention_weights, axis=0)\n",
    "    print(f\"Average Attention Weights - EEG: {avg_attention_weights[0][0]:.4f}, EMG: {avg_attention_weights[1][0]:.4f}\")\n",
    "    return avg_attention_weights\n",
    "\n",
    "def plot_modality_contribution(attention_weights):\n",
    "\n",
    "    modalities = ['EEG', 'EMG']\n",
    "    contributions = attention_weights.flatten()  # Flatten the attention weights array\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x=modalities, y=contributions, palette=\"viridis\")\n",
    "    plt.title('Modality Contribution Based on Attention Weights')\n",
    "    plt.ylabel('Contribution Weight')\n",
    "    plt.xlabel('Modality')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, val_loader, train_losses=None, val_losses=None):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_scores = []  # For probability outputs\n",
    "    attention_weights = []  # To store attention weights for contribution analysis\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            eeg = X_batch[:, :input_dim].unsqueeze(1)\n",
    "            emg = X_batch[:, input_dim:].unsqueeze(1)\n",
    "            outputs, weights = model(eeg, emg)  # Get attention weights\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_scores.extend(probs.cpu().numpy())\n",
    "            attention_weights.append(weights.cpu().numpy())\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_scores = np.array(y_scores)\n",
    "    attention_weights = np.array(attention_weights)  # Convert to NumPy array\n",
    "\n",
    "    # Compute classification metrics\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    log_loss_value = log_loss(y_true, y_scores)\n",
    "    hamming_loss_value = calculate_hamming_loss(y_true, y_pred)\n",
    "    top_k_accuracy_value = calculate_top_k_accuracy(y_true, y_scores, k=3)\n",
    "\n",
    "    # AUROC and AUPRC (only for binary or multiclass with probability scores)\n",
    "    try:\n",
    "        auroc = roc_auc_score(y_true, y_scores, multi_class='ovr', average='weighted')\n",
    "        auprc = average_precision_score(y_true, y_scores, average='weighted')\n",
    "    except ValueError:\n",
    "        auroc = None\n",
    "        auprc = None\n",
    "\n",
    "    # Mean Per-Class Error (MPCE)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    per_class_error = 1 - (np.diag(cm) / cm.sum(axis=1))\n",
    "    mpce = np.mean(per_class_error)\n",
    "\n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_acc:.2f}, Cohen's Kappa: {kappa:.2f}, MCC: {mcc:.2f}\")\n",
    "    print(f\"Log Loss: {log_loss_value:.4f}, \"\n",
    "          f\"AUROC: {auroc:.2f}\" if auroc is not None else \"AUROC: N/A\",\n",
    "          f\"AUPRC: {auprc:.2f}\" if auprc is not None else \"AUPRC: N/A\")\n",
    "    print(f\"MPCE: {mpce:.4f}\")\n",
    "    print(f\"Hamming Loss: {hamming_loss_value:.4f}\")\n",
    "    print(f\"Top-3 Accuracy: {top_k_accuracy_value:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC and Precision-Recall curves\n",
    "    plot_roc_curve(y_true, y_scores, num_classes=len(np.unique(y_true)))\n",
    "    plot_precision_recall_curve(y_true, y_scores, num_classes=len(np.unique(y_true)))\n",
    "\n",
    "    # Contribution analysis: EEG vs. EMG\n",
    "    avg_attention_weights = analyze_attention_weights(attention_weights)\n",
    "\n",
    "    # Plot modality contribution\n",
    "    plot_modality_contribution(avg_attention_weights)\n",
    "\n",
    "    # Ablation studies\n",
    "    ablation_study(model, val_loader, ablation_type='eeg')\n",
    "    ablation_study(model, val_loader, ablation_type='emg')\n",
    "\n",
    "    # Computational efficiency metrics\n",
    "    inference_time = measure_inference_time(model, val_loader)\n",
    "    model_size = measure_model_size(model)\n",
    "\n",
    "    # Return all relevant metrics and data\n",
    "    metrics = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'cohen_kappa': kappa,\n",
    "        'mcc': mcc,\n",
    "        'log_loss': log_loss_value,\n",
    "        'auroc': auroc,\n",
    "        'auprc': auprc,\n",
    "        'mpce': mpce,\n",
    "        'hamming_loss': hamming_loss_value,\n",
    "        'top_k_accuracy': top_k_accuracy_value,\n",
    "        'sync_score': sync_score,\n",
    "        'attention_weights': avg_attention_weights,\n",
    "        'inference_time': inference_time,\n",
    "        'model_size': model_size,\n",
    "        'y_true': y_true,  # Add y_true to the returned dictionary\n",
    "        'y_scores': y_scores,  # Add y_scores to the returned dictionary\n",
    "        'confusion_matrix': cm  # Add confusion matrix to the returned dictionary\n",
    "    }\n",
    "\n",
    "    # Add train_losses and val_losses if provided\n",
    "    if train_losses is not None:\n",
    "        metrics['train_losses'] = train_losses\n",
    "    if val_losses is not None:\n",
    "        metrics['val_losses'] = val_losses\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def train_model_with_weight_decay(model, train_loader, val_loader, criterion, epochs=15, patience=5, lr=0.0005):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []  # To store training losses for each epoch\n",
    "    val_losses = []    # To store validation losses for each epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            eeg = X_batch[:, :input_dim].unsqueeze(1)\n",
    "            emg = X_batch[:, input_dim:].unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(eeg, emg)  # Ignore attention weights during training\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Record average training loss for the epoch\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                eeg = X_batch[:, :input_dim].unsqueeze(1)\n",
    "                emg = X_batch[:, input_dim:].unsqueeze(1)\n",
    "                outputs, _ = model(eeg, emg)  # Ignore attention weights during validation\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Record average validation loss for the epoch\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    # Return training and validation losses\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "# Initialize lists to store aggregated data across folds\n",
    "all_y_true = []\n",
    "all_y_scores = []\n",
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "all_confusion_matrices = []\n",
    "\n",
    "# k-Fold Cross-Validation with Online Adaptation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "before_adaptation_metrics = []  # To store metrics before online adaptation\n",
    "after_adaptation_metrics = []   # To store metrics after online adaptation\n",
    "online_adaptation_percentage = 0.3  # 30% data for online adaptation\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(combined_data)):\n",
    "    print(f\"Fold {fold + 1}/{k}\")\n",
    "    X_train, X_val = combined_data.values[train_index], combined_data.values[val_index]\n",
    "    y_train, y_val = labels[train_index], labels[val_index]\n",
    "    \n",
    "    train_dataset = EEGEMGDataset(X_train, y_train)\n",
    "    val_dataset = EEGEMGDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    input_dim = X_train.shape[1] // 2\n",
    "    hidden_dim = 256\n",
    "    num_heads = 4\n",
    "    num_classes = len(np.unique(labels))\n",
    "    \n",
    "    model = EEGEMGTransformer(input_dim=input_dim, hidden_dim=hidden_dim, num_heads=num_heads, num_classes=num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train model and get training/validation losses\n",
    "    train_losses, val_losses = train_model_with_weight_decay(model, train_loader, val_loader, criterion)\n",
    "\n",
    "    print(\"Before Online Adaptation:\")\n",
    "    metrics_before = evaluate_model(model, val_loader, train_losses=train_losses, val_losses=val_losses)  # Pass losses here\n",
    "    before_adaptation_metrics.append(metrics_before)  # Store metrics before adaptation\n",
    "\n",
    "    # Collect data for average plots\n",
    "    all_y_true.extend(metrics_before['y_true'])\n",
    "    all_y_scores.extend(metrics_before['y_scores'])\n",
    "    all_train_losses.append(metrics_before['train_losses'])\n",
    "    all_val_losses.append(metrics_before['val_losses'])\n",
    "    all_confusion_matrices.append(metrics_before['confusion_matrix'])\n",
    "\n",
    "    # Online adaptation\n",
    "    online_data_size = int(len(X_val) * online_adaptation_percentage)\n",
    "    online_X, online_y = X_val[:online_data_size], y_val[:online_data_size]\n",
    "    print(f\"Online Adaptation using {online_data_size} samples.\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.000001, weight_decay=0.01)\n",
    "    online_adaptation_with_regularization(\n",
    "        model, optimizer, online_X, online_y, criterion, val_loader, num_cycles=5, batch_size=16, lr=0.00001\n",
    "    )\n",
    "\n",
    "    print(\"After Online Adaptation:\")\n",
    "    metrics_after = evaluate_model(model, val_loader)\n",
    "    after_adaptation_metrics.append(metrics_after)  # Store metrics after adaptation\n",
    "\n",
    "    # Combine metrics for this fold\n",
    "    fold_results.append({\n",
    "        'before_adaptation': metrics_before,\n",
    "        'after_adaptation': metrics_after\n",
    "    })\n",
    "\n",
    "# Debugging: Print metrics before and after adaptation\n",
    "print(\"Before Adaptation Metrics:\")\n",
    "for i, metrics in enumerate(before_adaptation_metrics):\n",
    "    print(f\"Fold {i + 1}:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"After Adaptation Metrics:\")\n",
    "for i, metrics in enumerate(after_adaptation_metrics):\n",
    "    print(f\"Fold {i + 1}:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "best_model_index = np.argmax([result['after_adaptation']['accuracy'] for result in fold_results])\n",
    "torch.save(model.state_dict(), 'EEGEMGTransformer_best.pth')\n",
    "print(f\"Best model (Fold {best_model_index + 1}) saved as EEGEMGTransformer_best.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29198e9-f1fc-4003-926b-7ece54d352f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310eaa93-9407-4dff-9db5-adf74e16a3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
